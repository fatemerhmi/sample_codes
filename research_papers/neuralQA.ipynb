{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# NeuralQA\n",
    "- NeuralQA: A Usable Library for Question Answering on Large Datasets with BERT \n",
    "- [documentation](https://victordibia.com/neuralqa/)\n",
    "- paper\n",
    "- [github](https://github.com/victordibia/neuralqa)\n",
    "- [Demo](https://neuralqa.fastforwardlabs.com/#/)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "steps to setup?\n",
    "- config.yaml file\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "reader:\n",
    "  title: Reader\n",
    "  selected: twmkn9/distilbert-base-uncased-squad2\n",
    "  options:\n",
    "    - name: DistilBERT SQUAD2\n",
    "      value: twmkn9/distilbert-base-uncased-squad2\n",
    "      type: distilbert\n",
    "    - name: BERT SQUAD2\n",
    "      value: deepset/bert-base-cased-squad2\n",
    "      type: bert\n",
    "  ```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BERTReader(Reader):\n",
    "    def __init__(self, model_name, model_path, model_type=\"bert\", **kwargs):\n",
    "        Reader.__init__(self, model_name, model_path, model_type)\n",
    "        # self.load_model(model_name, model_path, model_type)\n",
    "\n",
    "    def get_best_start_end_position(self, start_scores, end_scores):\n",
    "        print(start_scores)\n",
    "        answer_start = tf.argmax(start_scores, axis=1).numpy()[0]\n",
    "        answer_end = (tf.argmax(end_scores, axis=1) + 1).numpy()[0]\n",
    "        return answer_start, answer_end\n",
    "\n",
    "    def get_chunk_answer_span(self, inputs):\n",
    "        start_time = time.time()\n",
    "        answer_start_scores, answer_end_scores = self.model(inputs)\n",
    "\n",
    "        answer_start, answer_end = self.get_best_start_end_position(\n",
    "            answer_start_scores, answer_end_scores)\n",
    "\n",
    "        answer_end = answer_end - \\\n",
    "            1 if answer_end == answer_end_scores.shape[1] else answer_end\n",
    "\n",
    "        answer_start_softmax_probability = tf.nn.softmax(\n",
    "            answer_start_scores, axis=1).numpy()[0][answer_start]\n",
    "        answer_end_softmax_probability = tf.nn.softmax(\n",
    "            answer_end_scores, axis=1).numpy()[0][answer_end]\n",
    "\n",
    "        answer = self.tokenizer.decode(\n",
    "            inputs[\"input_ids\"][0][answer_start:answer_end], skip_special_tokens=True)\n",
    "\n",
    "        # if model predict first token 0 which is in the question as part of the answer, return nothing\n",
    "        if answer_start == 0:\n",
    "            answer = \"\"\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        return {\"answer\": answer, \"took\": elapsed_time,\n",
    "                \"start_probability\": str(answer_start_softmax_probability),\n",
    "                \"end_probability\": str(answer_end_softmax_probability),\n",
    "                \"probability\": str(answer_end_softmax_probability + answer_start_softmax_probability / 2)\n",
    "                }\n",
    "\n",
    "    def token_chunker(self, question, context, max_chunk_size=512, stride=2, max_num_chunks=5):\n",
    "        # we tokenize question and context once.\n",
    "        # if question + context > max chunksize, we break it down into multiple chunks of question +\n",
    "        # subsets of context with some stride overlap\n",
    "\n",
    "        question_tokens = self.tokenizer.encode(question)\n",
    "        context_tokens = self.tokenizer.encode(\n",
    "            context, add_special_tokens=False)\n",
    "\n",
    "        chunk_holder = []\n",
    "        chunk_size = max_chunk_size - len(question_tokens) - 1\n",
    "        # -1 for the 102 end token we append later\n",
    "        current_pos = 0\n",
    "        chunk_count = 0\n",
    "        while current_pos < len(context_tokens) and current_pos >= 0:\n",
    "\n",
    "            # we want to cap the number of chunks we create\n",
    "            if max_num_chunks and chunk_count >= max_num_chunks:\n",
    "                break\n",
    "\n",
    "            end_point = current_pos + \\\n",
    "                chunk_size if (current_pos + chunk_size) < len(context_tokens) - \\\n",
    "                1 else len(context_tokens) - 1\n",
    "            token_chunk = question_tokens + \\\n",
    "                context_tokens[current_pos: end_point] + [102]\n",
    "\n",
    "            # question type is 0, context type is 1, convert to tf\n",
    "            token_type_ids = [0]*len(question_tokens) + \\\n",
    "                [1] * (len(token_chunk) - len(question_tokens))\n",
    "            token_type_ids = tf.constant(\n",
    "                token_type_ids, dtype='int32', shape=(1, len(token_type_ids)))\n",
    "\n",
    "            # attend to every token\n",
    "            attention_mask = tf.ones(\n",
    "                (1, len(token_chunk)),  dtype=tf.dtypes.int32)\n",
    "\n",
    "            # convert token chunk to tf\n",
    "            token_chunk = tf.constant(\n",
    "                token_chunk, dtype='int32', shape=(1, len(token_chunk)))\n",
    "\n",
    "            chunk_holder.append(\n",
    "                {\"token_ids\": token_chunk,\n",
    "                 \"context\": self.tokenizer.decode(context_tokens[current_pos: end_point], skip_special_tokens=True),\n",
    "                 \"attention_mask\":  attention_mask,\n",
    "                 \"token_type_ids\": token_type_ids\n",
    "                 })\n",
    "            current_pos = current_pos + chunk_size - stride + 1\n",
    "            chunk_count += 1\n",
    "\n",
    "        return chunk_holder\n",
    "\n",
    "    def answer_question(self, question, context, max_chunk_size=512, stride=70):\n",
    "\n",
    "        # chunk tokens\n",
    "        chunked_tokens = self.token_chunker(\n",
    "            question, context, max_chunk_size, stride)\n",
    "        answer_holder = []\n",
    "        for chunk in chunked_tokens:\n",
    "            model_input = {\"input_ids\": chunk[\"token_ids\"], \"attention_mask\":\n",
    "                           chunk[\"attention_mask\"], \"token_type_ids\": chunk[\"token_type_ids\"]}\n",
    "            answer = self.get_chunk_answer_span(model_input)\n",
    "            if len(answer[\"answer\"]) > 2:\n",
    "                answer[\"question\"] = question\n",
    "                answer[\"context\"] = chunk[\"context\"].replace(\"##\", \"\").replace(\n",
    "                    answer[\"answer\"], \" <em>\" + answer[\"answer\"] + \"</em> \")\n",
    "                answer_holder.append(answer)\n",
    "        return answer_holder\n",
    "\n",
    "    def get_correct_span_mask(self, correct_index, token_size):\n",
    "        span_mask = np.zeros((1, token_size))\n",
    "        span_mask[0, correct_index] = 1\n",
    "        span_mask = tf.constant(span_mask, dtype='float32')\n",
    "        return span_mask\n",
    "\n",
    "    def get_embedding_matrix(self):\n",
    "        if \"DistilBert\" in type(self.model).__name__:\n",
    "            return self.model.distilbert.embeddings.word_embeddings\n",
    "        else:\n",
    "            return self.model.bert.embeddings.word_embeddings\n",
    "\n",
    "    # move this to some utils file\n",
    "    def clean_tokens(self, gradients, tokens, token_types):\n",
    "        \"\"\"\n",
    "        Clean the tokens and  gradients\n",
    "        Remove \"[CLS]\",\"[CLR]\", \"[SEP]\" tokens\n",
    "        Reduce (mean) gradients values for tokens that are split ##\n",
    "        \"\"\"\n",
    "        token_holder = []\n",
    "        token_type_holder = []\n",
    "        gradient_holder = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if (tokens[i] not in [\"[CLS]\", \"[CLR]\", \"[SEP]\"]):\n",
    "                token = tokens[i]\n",
    "                conn = gradients[i]\n",
    "                token_type = token_types[i]\n",
    "                if i < len(tokens)-1:\n",
    "                    if tokens[i+1][0:2] == \"##\":\n",
    "                        token = tokens[i]\n",
    "                        conn = gradients[i]\n",
    "                        j = 1\n",
    "                        while i < len(tokens)-1 and tokens[i+1][0:2] == \"##\":\n",
    "                            i += 1\n",
    "                            token += tokens[i][2:]\n",
    "                            conn += gradients[i]\n",
    "                            j += 1\n",
    "                        conn = conn / j\n",
    "                token_holder.append(token)\n",
    "                token_type_holder.append(token_type)\n",
    "                # gradient_holder.append(conn)\n",
    "                gradient_holder.append(\n",
    "                    {\"gradient\": conn, \"token\": token, \"token_type\": token_type})\n",
    "            i += 1\n",
    "        return gradient_holder\n",
    "\n",
    "    def get_gradient(self, question, context):\n",
    "        \"\"\"Return gradient of input (question) wrt to model output span prediction\n",
    "        Args:\n",
    "            question (str): text of input question\n",
    "            context (str): text of question context/passage\n",
    "            model (QA model): Hugging Face BERT model for QA transformers.modeling_tf_distilbert.TFDistilBertForQuestionAnswering, transformers.modeling_tf_bert.TFBertForQuestionAnswering\n",
    "            tokenizer (tokenizer): transformers.tokenization_bert.BertTokenizerFast \n",
    "        Returns:\n",
    "            (tuple): (gradients, token_words, token_types, answer_text)\n",
    "        \"\"\"\n",
    "\n",
    "        embedding_matrix = self.get_embedding_matrix()\n",
    "\n",
    "        encoded_tokens = self.tokenizer.encode_plus(\n",
    "            question, context, add_special_tokens=True, return_token_type_ids=True, return_tensors=\"tf\")\n",
    "        token_ids = list(encoded_tokens[\"input_ids\"].numpy()[0])\n",
    "        vocab_size = embedding_matrix.get_shape()[0]\n",
    "\n",
    "        # convert token ids to one hot. We can't differentiate wrt to int token ids hence the need for one hot representation\n",
    "        token_ids_tensor = tf.constant([token_ids], dtype='int32')\n",
    "        token_ids_tensor_one_hot = tf.one_hot(token_ids_tensor, vocab_size)\n",
    "\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            # (i) watch input variable\n",
    "            tape.watch(token_ids_tensor_one_hot)\n",
    "\n",
    "            # multiply input model embedding matrix; allows us do backprop wrt one hot input\n",
    "            inputs_embeds = tf.matmul(\n",
    "                token_ids_tensor_one_hot, embedding_matrix)\n",
    "\n",
    "            # (ii) get prediction\n",
    "            start_scores, end_scores = self.model(\n",
    "                {\"inputs_embeds\": inputs_embeds, \"token_type_ids\": encoded_tokens[\"token_type_ids\"], \"attention_mask\": encoded_tokens[\"attention_mask\"]})\n",
    "            answer_start, answer_end = self.get_best_start_end_position(\n",
    "                start_scores, end_scores)\n",
    "\n",
    "            start_output_mask = self.get_correct_span_mask(\n",
    "                answer_start, len(token_ids))\n",
    "            end_output_mask = self.get_correct_span_mask(\n",
    "                answer_end, len(token_ids))\n",
    "\n",
    "            # zero out all predictions outside of the correct span positions; we want to get gradients wrt to just these positions\n",
    "            predict_correct_start_token = tf.reduce_sum(\n",
    "                start_scores * start_output_mask)\n",
    "            predict_correct_end_token = tf.reduce_sum(\n",
    "                end_scores * end_output_mask)\n",
    "\n",
    "            # (iii) get gradient of input with respect to both start and end output\n",
    "            gradient_non_normalized = tf.norm(\n",
    "                tape.gradient([predict_correct_start_token, predict_correct_end_token], token_ids_tensor_one_hot), axis=2)\n",
    "\n",
    "            # (iv) normalize gradient scores and return them as \"explanations\"\n",
    "            gradient_tensor = (\n",
    "                gradient_non_normalized /\n",
    "                tf.reduce_max(gradient_non_normalized)\n",
    "            )\n",
    "            gradients = gradient_tensor[0].numpy().tolist()\n",
    "\n",
    "            token_words = self.tokenizer.convert_ids_to_tokens(token_ids)\n",
    "            token_types = list(\n",
    "                encoded_tokens[\"token_type_ids\"].numpy()[0].tolist())\n",
    "            answer_text = self.tokenizer.decode(\n",
    "                token_ids[answer_start:answer_end],  skip_special_tokens=True)\n",
    "\n",
    "            # clean up gradients and words\n",
    "            gradients = self.clean_tokens(\n",
    "                gradients, token_words, token_types)\n",
    "            return gradients, answer_text, question\n",
    "\n",
    "    def explain_model(self, question, context, explain_method=\"gradient\"):\n",
    "        if explain_method == \"gradient\":\n",
    "            return self.get_gradient(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from neuralqa.reader import BERTReader\n",
    "class ReaderPool():\n",
    "    def __init__(self, models):\n",
    "        self._selected_model = models[\"selected\"]\n",
    "        self.reader_pool = {}\n",
    "        for model in models[\"options\"]:\n",
    "            if (model[\"type\"] == \"bert\" or model[\"type\"] == \"distilbert\"):\n",
    "                self.reader_pool[model[\"value\"]] = BERTReader(\n",
    "                    model[\"name\"], model[\"value\"])\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.reader_pool[self.selected_model]\n",
    "\n",
    "    @property\n",
    "    def selected_model(self):\n",
    "        return self._selected_model\n",
    "\n",
    "    @selected_model.setter\n",
    "    def selected_model(self, selected_model):\n",
    "\n",
    "        if (selected_model in self.reader_pool):\n",
    "            self._selected_model = selected_model\n",
    "        else:\n",
    "            if (len(self.reader_pool) > 0):\n",
    "                default_model = next(iter(self.reader_pool))\n",
    "                logger.info(\n",
    "                    \">> Model you are attempting to use %s does not exist in model pool. Using the following default model instead %s \", selected_model, default_model)\n",
    "                self._selected_model = default_model\n",
    "            else:\n",
    "                logger.info(\n",
    "                    \">> No reader has been specified in config.yaml.\")\n",
    "                self._selected_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reader.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
    "import time\n",
    "import logging\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Reader:\n",
    "    def __init__(self, model_name, model_path, model_type, **kwargs):\n",
    "        self.load_model(model_name, model_path, model_type)\n",
    "\n",
    "    def load_model(self, model_name, model_path, model_type):\n",
    "        logger.info(\">> Loading HF model \" +\n",
    "                    model_name + \" from \" + model_path)\n",
    "        self.type = model_type\n",
    "        self.name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path, use_fast=True)\n",
    "        self.model = TFAutoModelForQuestionAnswering.from_pretrained(\n",
    "            model_path, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "class Answer(BaseModel):\n",
    "\n",
    "    max_documents: Optional[int] = 5\n",
    "    query: str = \"what is a fourth amendment right violation?\"\n",
    "    fragment_size: int = 250\n",
    "    tokenstride: int = 50\n",
    "    context: Optional[str] = \"The fourth amendment kind of protects the rights of citizens .. such that they dont get searched\"\n",
    "    reader: str = None\n",
    "    relsnip: bool = True\n",
    "    expander: Optional[str] = None\n",
    "    expansionterms: Optional[list] = None\n",
    "    retriever: Optional[str] = \"manual\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n",
      "{'question': 'what is the goal of the fourth amendment?', 'context': \"The Fourth Amendment of the U.S. Constitution provides that the right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures, shall not be violated, and no Warrants shall issue, but upon probable cause, supported by Oath or affirmation, and particularly describing the place to be searched, and the persons or things to be seized.'The ultimate goal of this provision is to protect people’s right to privacy and freedom from unreasonable intrusions by the government. However, the Fourth Amendment does not guarantee protection from all searches and seizures, but only those done by the government and deemed unreasonable under the law.\"}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "config_file = \"config.yaml\"\n",
    "with open(config_file, 'r') as f:\n",
    "        app_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "reader_pool = ReaderPool(app_config[\"reader\"])\n",
    "\n",
    "print(app_config[\"samples\"][0])\n",
    "# params = Answer()\n",
    "# # answer question based on provided context\n",
    "# answers = reader_pool.model.answer_question(params.query, params.context, stride=params.tokenstride)\n",
    "# for answer in answers:\n",
    "#         answer[\"index\"] = 0\n",
    "#         answer_holder.append(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}