{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Language model training\n",
    "[source](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/README.md)\n",
    "Fine-tuning (or training from scratch) the library models for language modeling on a text dataset\n",
    "* GPT and GPT-2 are trained or fine-tuned using a causal language modeling (CLM) loss\n",
    "* ALBERT, BERT, DistilBERT and RoBERTa are trained or fine-tuned using a masked language modeling (MLM) loss \n",
    "* XLNet uses permutation language modeling (PLM)\n",
    "\n",
    "\n",
    "find more information about the differences between those objectives in [model summary](https://huggingface.co/transformers/model_summary.html)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## RoBERTa/BERT/DistilBERT and masked language modeling\n",
    "\n",
    "To use this CLI tool follow these steps:       \n",
    "1. `git clone https://github.com/huggingface/transformers` (Note that you have to **install the library from source**)    \n",
    "2. `cd transformers`    \n",
    "3. `pip install .`   \n",
    "4. `cd examples/language-modeling`    \n",
    "5. `pip install -r requirements.txt`  \n",
    "6. `git checkout tags/v3.5.1`   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-02-04 11:17:33.586101: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "usage: run_mlm.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "                  [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]\n",
      "                  [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
      "                  [--no_use_fast_tokenizer] [--dataset_name DATASET_NAME]\n",
      "                  [--dataset_config_name DATASET_CONFIG_NAME]\n",
      "                  [--train_file TRAIN_FILE]\n",
      "                  [--validation_file VALIDATION_FILE] [--overwrite_cache]\n",
      "                  [--max_seq_length MAX_SEQ_LENGTH]\n",
      "                  [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]\n",
      "                  [--mlm_probability MLM_PROBABILITY] [--line_by_line]\n",
      "                  [--pad_to_max_length] --output_dir OUTPUT_DIR\n",
      "                  [--overwrite_output_dir] [--do_train] [--do_eval]\n",
      "                  [--do_predict]\n",
      "                  [--evaluation_strategy {EvaluationStrategy.NO,EvaluationStrategy.STEPS,EvaluationStrategy.EPOCH}]\n",
      "                  [--prediction_loss_only]\n",
      "                  [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                  [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                  [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                  [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                  [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                  [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
      "                  [--learning_rate LEARNING_RATE]\n",
      "                  [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n",
      "                  [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n",
      "                  [--max_grad_norm MAX_GRAD_NORM]\n",
      "                  [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                  [--max_steps MAX_STEPS]\n",
      "                  [--lr_scheduler_type {SchedulerType.LINEAR,SchedulerType.COSINE,SchedulerType.COSINE_WITH_RESTARTS,SchedulerType.POLYNOMIAL,SchedulerType.CONSTANT,SchedulerType.CONSTANT_WITH_WARMUP}]\n",
      "                  [--warmup_steps WARMUP_STEPS] [--logging_dir LOGGING_DIR]\n",
      "                  [--logging_first_step] [--logging_steps LOGGING_STEPS]\n",
      "                  [--save_steps SAVE_STEPS]\n",
      "                  [--save_total_limit SAVE_TOTAL_LIMIT] [--no_cuda]\n",
      "                  [--seed SEED] [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                  [--fp16_backend {auto,amp,apex}] [--local_rank LOCAL_RANK]\n",
      "                  [--tpu_num_cores TPU_NUM_CORES] [--tpu_metrics_debug]\n",
      "                  [--debug] [--dataloader_drop_last] [--eval_steps EVAL_STEPS]\n",
      "                  [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                  [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
      "                  [--disable_tqdm DISABLE_TQDM] [--no_remove_unused_columns]\n",
      "                  [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
      "                  [--load_best_model_at_end]\n",
      "                  [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
      "                  [--greater_is_better GREATER_IS_BETTER] [--ignore_data_skip]\n",
      "                  [--sharded_ddp] [--deepspeed DEEPSPEED]\n",
      "                  [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
      "                  [--adafactor]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model_name_or_path MODEL_NAME_OR_PATH\n",
      "                        The model checkpoint for weights initialization.Don't\n",
      "                        set if you want to train a model from scratch.\n",
      "  --model_type MODEL_TYPE\n",
      "                        If training from scratch, pass a model type from the\n",
      "                        list: layoutlm, distilbert, albert, bart, camembert,\n",
      "                        xlm-roberta, longformer, roberta, squeezebert, bert,\n",
      "                        mobilebert, flaubert, xlm, electra, reformer, funnel,\n",
      "                        mpnet, tapas\n",
      "  --config_name CONFIG_NAME\n",
      "                        Pretrained config name or path if not the same as\n",
      "                        model_name\n",
      "  --tokenizer_name TOKENIZER_NAME\n",
      "                        Pretrained tokenizer name or path if not the same as\n",
      "                        model_name\n",
      "  --cache_dir CACHE_DIR\n",
      "                        Where do you want to store the pretrained models\n",
      "                        downloaded from s3\n",
      "  --no_use_fast_tokenizer\n",
      "                        Whether to use one of the fast tokenizer (backed by\n",
      "                        the tokenizers library) or not.\n",
      "  --dataset_name DATASET_NAME\n",
      "                        The name of the dataset to use (via the datasets\n",
      "                        library).\n",
      "  --dataset_config_name DATASET_CONFIG_NAME\n",
      "                        The configuration name of the dataset to use (via the\n",
      "                        datasets library).\n",
      "  --train_file TRAIN_FILE\n",
      "                        The input training data file (a text file).\n",
      "  --validation_file VALIDATION_FILE\n",
      "                        An optional input evaluation data file to evaluate the\n",
      "                        perplexity on (a text file).\n",
      "  --overwrite_cache     Overwrite the cached training and evaluation sets\n",
      "  --max_seq_length MAX_SEQ_LENGTH\n",
      "                        The maximum total input sequence length after\n",
      "                        tokenization. Sequences longer than this will be\n",
      "                        truncated.\n",
      "  --preprocessing_num_workers PREPROCESSING_NUM_WORKERS\n",
      "                        The number of processes to use for the preprocessing.\n",
      "  --mlm_probability MLM_PROBABILITY\n",
      "                        Ratio of tokens to mask for masked language modeling\n",
      "                        loss\n",
      "  --line_by_line        Whether distinct lines of text in the dataset are to\n",
      "                        be handled as distinct sequences.\n",
      "  --pad_to_max_length   Whether to pad all samples to `max_seq_length`. If\n",
      "                        False, will pad the samples dynamically when batching\n",
      "                        to the maximum length in the batch.\n",
      "  --output_dir OUTPUT_DIR\n",
      "                        The output directory where the model predictions and\n",
      "                        checkpoints will be written.\n",
      "  --overwrite_output_dir\n",
      "                        Overwrite the content of the output directory.Use this\n",
      "                        to continue training if output_dir points to a\n",
      "                        checkpoint directory.\n",
      "  --do_train            Whether to run training.\n",
      "  --do_eval             Whether to run eval on the dev set.\n",
      "  --do_predict          Whether to run predictions on the test set.\n",
      "  --evaluation_strategy {EvaluationStrategy.NO,EvaluationStrategy.STEPS,EvaluationStrategy.EPOCH}\n",
      "                        The evaluation strategy to use.\n",
      "  --prediction_loss_only\n",
      "                        When performing evaluation and predictions, only\n",
      "                        returns the loss.\n",
      "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n",
      "                        Batch size per GPU/TPU core/CPU for training.\n",
      "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n",
      "                        Batch size per GPU/TPU core/CPU for evaluation.\n",
      "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
      "                        Deprecated, the use of `--per_device_train_batch_size`\n",
      "                        is preferred. Batch size per GPU/TPU core/CPU for\n",
      "                        training.\n",
      "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
      "                        Deprecated, the use of `--per_device_eval_batch_size`\n",
      "                        is preferred.Batch size per GPU/TPU core/CPU for\n",
      "                        evaluation.\n",
      "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
      "                        Number of updates steps to accumulate before\n",
      "                        performing a backward/update pass.\n",
      "  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\n",
      "                        Number of predictions steps to accumulate before\n",
      "                        moving the tensors to the CPU.\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        The initial learning rate for Adam.\n",
      "  --weight_decay WEIGHT_DECAY\n",
      "                        Weight decay if we apply some.\n",
      "  --adam_beta1 ADAM_BETA1\n",
      "                        Beta1 for Adam optimizer\n",
      "  --adam_beta2 ADAM_BETA2\n",
      "                        Beta2 for Adam optimizer\n",
      "  --adam_epsilon ADAM_EPSILON\n",
      "                        Epsilon for Adam optimizer.\n",
      "  --max_grad_norm MAX_GRAD_NORM\n",
      "                        Max gradient norm.\n",
      "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
      "                        Total number of training epochs to perform.\n",
      "  --max_steps MAX_STEPS\n",
      "                        If > 0: set total number of training steps to perform.\n",
      "                        Override num_train_epochs.\n",
      "  --lr_scheduler_type {SchedulerType.LINEAR,SchedulerType.COSINE,SchedulerType.COSINE_WITH_RESTARTS,SchedulerType.POLYNOMIAL,SchedulerType.CONSTANT,SchedulerType.CONSTANT_WITH_WARMUP}\n",
      "                        The scheduler type to use.\n",
      "  --warmup_steps WARMUP_STEPS\n",
      "                        Linear warmup over warmup_steps.\n",
      "  --logging_dir LOGGING_DIR\n",
      "                        Tensorboard log dir.\n",
      "  --logging_first_step  Log the first global_step\n",
      "  --logging_steps LOGGING_STEPS\n",
      "                        Log every X updates steps.\n",
      "  --save_steps SAVE_STEPS\n",
      "                        Save checkpoint every X updates steps.\n",
      "  --save_total_limit SAVE_TOTAL_LIMIT\n",
      "                        Limit the total amount of checkpoints.Deletes the\n",
      "                        older checkpoints in the output_dir. Default is\n",
      "                        unlimited checkpoints\n",
      "  --no_cuda             Do not use CUDA even when it is available\n",
      "  --seed SEED           random seed for initialization\n",
      "  --fp16                Whether to use 16-bit (mixed) precision (through\n",
      "                        NVIDIA Apex) instead of 32-bit\n",
      "  --fp16_opt_level FP16_OPT_LEVEL\n",
      "                        For fp16: Apex AMP optimization level selected in\n",
      "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
      "                        https://nvidia.github.io/apex/amp.html\n",
      "  --fp16_backend {auto,amp,apex}\n",
      "                        The backend to be used for mixed precision.\n",
      "  --local_rank LOCAL_RANK\n",
      "                        For distributed training: local_rank\n",
      "  --tpu_num_cores TPU_NUM_CORES\n",
      "                        TPU: Number of TPU cores (automatically passed by\n",
      "                        launcher script)\n",
      "  --tpu_metrics_debug   Deprecated, the use of `--debug` is preferred. TPU:\n",
      "                        Whether to print debug metrics\n",
      "  --debug               Whether to print debug metrics on TPU\n",
      "  --dataloader_drop_last\n",
      "                        Drop the last incomplete batch if it is not divisible\n",
      "                        by the batch size.\n",
      "  --eval_steps EVAL_STEPS\n",
      "                        Run an evaluation every X steps.\n",
      "  --dataloader_num_workers DATALOADER_NUM_WORKERS\n",
      "                        Number of subprocesses to use for data loading\n",
      "                        (PyTorch only). 0 means that the data will be loaded\n",
      "                        in the main process.\n",
      "  --past_index PAST_INDEX\n",
      "                        If >=0, uses the corresponding part of the output as\n",
      "                        the past state for next step.\n",
      "  --run_name RUN_NAME   An optional descriptor for the run. Notably used for\n",
      "                        wandb logging.\n",
      "  --disable_tqdm DISABLE_TQDM\n",
      "                        Whether or not to disable the tqdm progress bars.\n",
      "  --no_remove_unused_columns\n",
      "                        Remove columns not required by the model when using an\n",
      "                        nlp.Dataset.\n",
      "  --label_names LABEL_NAMES [LABEL_NAMES ...]\n",
      "                        The list of keys in your dictionary of inputs that\n",
      "                        correspond to the labels.\n",
      "  --load_best_model_at_end\n",
      "                        Whether or not to load the best model found during\n",
      "                        training at the end of training.\n",
      "  --metric_for_best_model METRIC_FOR_BEST_MODEL\n",
      "                        The metric to use to compare two different models.\n",
      "  --greater_is_better GREATER_IS_BETTER\n",
      "                        Whether the `metric_for_best_model` should be\n",
      "                        maximized or not.\n",
      "  --ignore_data_skip    When resuming training, whether or not to skip the\n",
      "                        first epochs and batches to get to the same training\n",
      "                        data.\n",
      "  --sharded_ddp         Whether or not to use sharded DDP training (in\n",
      "                        distributed training only).\n",
      "  --deepspeed DEEPSPEED\n",
      "                        Enable deepspeed and pass the path to deepspeed json\n",
      "                        config file (e.g. ds_config.json)\n",
      "  --label_smoothing_factor LABEL_SMOOTHING_FACTOR\n",
      "                        The label smoothing epsilon to apply (zero means no\n",
      "                        label smoothing).\n",
      "  --adafactor           Whether or not to replace Adam by Adafactor.\n"
     ]
    }
   ],
   "source": [
    "!python3 ../transformers/examples/language-modeling/run_mlm.py --help"
   ]
  },
  {
   "source": [
    "Let's download a French dataset and train it with cammem-base"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset allocine_dataset (/users/grad/frahimi/.cache/huggingface/datasets/allocine_dataset/allocine/1.0.0/bbee2ebb45a067891973b91ebdd40a93598d1e2dd5710b6714cdc2cd81d0ed65)\n",
      "Reusing dataset allocine_dataset (/users/grad/frahimi/.cache/huggingface/datasets/allocine_dataset/allocine/1.0.0/bbee2ebb45a067891973b91ebdd40a93598d1e2dd5710b6714cdc2cd81d0ed65)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"allocine\", split='train')\n",
    "\n",
    "allocine_txt = \" \" .join(dataset['review'])\n",
    "\n",
    "f = open(\"allocine.txt\", \"a\")\n",
    "f.write(allocine_txt)\n",
    "f.close()\n",
    "\n",
    "dataset_validation = load_dataset(\"allocine\", split='validation')\n",
    "\n",
    "validation_txt = \" \" .join(dataset_validation['review'])\n",
    "# len(validation_txt)\n",
    "\n",
    "ff = open(\"validation.txt\", \"a\")\n",
    "ff.write(validation_txt)\n",
    "ff.close()"
   ]
  },
  {
   "source": [
    "To run on your own training and validation files, use the following command:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-02-04 11:55:43.925851: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "02/04/2021 11:55:45 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: False\n",
      "02/04/2021 11:55:45 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/tmp/test-mlm, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb04_11-55-45_selene, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/tmp/test-mlm, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, _n_gpu=2)\n",
      "Downloading: 2.57kB [00:00, 816kB/s]                                            \n",
      "Using custom data configuration default\n",
      "Downloading and preparing dataset text/default-a4263d271ebf3a8a (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /users/grad/frahimi/.cache/huggingface/datasets/text/default-a4263d271ebf3a8a/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
      "Dataset text downloaded and prepared to /users/grad/frahimi/.cache/huggingface/datasets/text/default-a4263d271ebf3a8a/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
      "loading configuration file https://huggingface.co/camembert-base/resolve/main/config.json from cache at /users/grad/frahimi/.cache/huggingface/transformers/f459e43c5ebb871abbf9209195563bff6a11547fd9532047739667c394833221.e23d229c54bcc6f67d337b8b2dd111b0e3dc01fa854bfecd3efdeb8c955749e6\n",
      "Model config CamembertConfig {\n",
      "  \"architectures\": [\n",
      "    \"CamembertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.2.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32005\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/camembert-base/resolve/main/config.json from cache at /users/grad/frahimi/.cache/huggingface/transformers/f459e43c5ebb871abbf9209195563bff6a11547fd9532047739667c394833221.e23d229c54bcc6f67d337b8b2dd111b0e3dc01fa854bfecd3efdeb8c955749e6\n",
      "Model config CamembertConfig {\n",
      "  \"architectures\": [\n",
      "    \"CamembertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 5,\n",
      "  \"eos_token_id\": 6,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.2.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32005\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/camembert-base/resolve/main/sentencepiece.bpe.model from cache at /users/grad/frahimi/.cache/huggingface/transformers/dbcb433aefd8b1a136d029fe2205a5c58a6336f8d3ba20e6c010f4d962174f5f.160b145acd37d2b3fd7c3694afcf4c805c2da5fd4ed4c9e4a23985e3c52ee452\n",
      "loading file https://huggingface.co/camembert-base/resolve/main/tokenizer.json from cache at /users/grad/frahimi/.cache/huggingface/transformers/84c442cc6020fc04ce266072af54b040f770850f629dd86c5951dbc23ac4c0dd.8fd2f10f70e05e6bf043e8a6947f6cdf9bb5dc937df6f9210a5c0ba8ee48e959\n",
      "loading weights file https://huggingface.co/camembert-base/resolve/main/pytorch_model.bin from cache at /users/grad/frahimi/.cache/huggingface/transformers/7e23f45751ad1fed420ca9f03bb37a279dc98a56c75bf25e671129237e2c893c.ee4d4253e08a7cf9697c0671fd8f022483dbf586691a7b32ead55493a34d72b2\n",
      "All model checkpoint weights were used when initializing CamembertForMaskedLM.\n",
      "\n",
      "Some weights of CamembertForMaskedLM were not initialized from the model checkpoint at camembert-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (20410091 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|████████████████████████████████████████████| 1/1 [03:01<00:00, 181.95s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:18<00:00, 18.45s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:50<00:00, 50.98s/ba]\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:06<00:00,  6.02s/ba]\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "***** Running training *****\n",
      "  Num examples = 39863\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7476\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  0%|                                                  | 0/7476 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"../transformers/examples/language-modeling/run_mlm.py\", line 386, in <module>\n",
      "    main()\n",
      "  File \"../transformers/examples/language-modeling/run_mlm.py\", line 355, in main\n",
      "    model_path=model_args.model_name_or_path if os.path.isdir(model_args.model_name_or_path) else None\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/transformers/trainer.py\", line 888, in train\n",
      "    tr_loss += self.training_step(model, inputs)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/transformers/trainer.py\", line 1250, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/transformers/trainer.py\", line 1277, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 155, in forward\n",
      "    outputs = self.parallel_apply(replicas, inputs, kwargs)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in parallel_apply\n",
      "    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in parallel_apply\n",
      "    output.reraise()\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/_utils.py\", line 395, in reraise\n",
      "    raise self.exc_type(msg)\n",
      "RuntimeError: Caught RuntimeError in replica 0 on device 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n",
      "    output = module(*input, **kwargs)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1043, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 808, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 505, in forward\n",
      "    output_attentions,\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 398, in forward\n",
      "    past_key_value=self_attn_past_key_value,\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 328, in forward\n",
      "    output_attentions,\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 251, in forward\n",
      "    attention_probs = self.dropout(attention_probs)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/nn/modules/dropout.py\", line 54, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "  File \"/users/grad/frahimi/.local/lib/python3.7/site-packages/torch/nn/functional.py\", line 936, in dropout\n",
      "    else _VF.dropout(input, p, training))\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 7.80 GiB total capacity; 4.67 GiB already allocated; 27.12 MiB free; 4.71 GiB reserved in total by PyTorch)\n",
      "\n",
      "  0%|                                                  | 0/7476 [00:05<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!python3 ../transformers/examples/language-modeling/run_mlm.py \\\n",
    "    --model_name_or_path camembert-base \\\n",
    "    --train_file allocine.txt \\\n",
    "    --validation_file validation.txt \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --output_dir /tmp/test-mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}