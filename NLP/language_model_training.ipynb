{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Language model training\n",
    "[source](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/README.md)\n",
    "Fine-tuning (or training from scratch) the library models for language modeling on a text dataset\n",
    "* GPT and GPT-2 are trained or fine-tuned using a causal language modeling (CLM) loss\n",
    "* ALBERT, BERT, DistilBERT and RoBERTa are trained or fine-tuned using a masked language modeling (MLM) loss \n",
    "* XLNet uses permutation language modeling (PLM)\n",
    "\n",
    "\n",
    "find more information about the differences between those objectives in [model summary](https://huggingface.co/transformers/model_summary.html)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## RoBERTa/BERT/DistilBERT and masked language modeling\n",
    "\n",
    "To use this CLI tool follow these steps:       \n",
    "1. `git clone https://github.com/huggingface/transformers` (Note that you have to **install the library from source**)    \n",
    "2. `cd transformers`    \n",
    "3. `pip install .`   \n",
    "4. `cd examples/language-modeling`    \n",
    "5. `pip install -r requirements.txt`  \n",
    "6. `git checkout tags/v3.5.1`   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-02-04 11:17:33.586101: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "usage: run_mlm.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "                  [--model_type MODEL_TYPE] [--config_name CONFIG_NAME]\n",
      "                  [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
      "                  [--no_use_fast_tokenizer] [--dataset_name DATASET_NAME]\n",
      "                  [--dataset_config_name DATASET_CONFIG_NAME]\n",
      "                  [--train_file TRAIN_FILE]\n",
      "                  [--validation_file VALIDATION_FILE] [--overwrite_cache]\n",
      "                  [--max_seq_length MAX_SEQ_LENGTH]\n",
      "                  [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]\n",
      "                  [--mlm_probability MLM_PROBABILITY] [--line_by_line]\n",
      "                  [--pad_to_max_length] --output_dir OUTPUT_DIR\n",
      "                  [--overwrite_output_dir] [--do_train] [--do_eval]\n",
      "                  [--do_predict]\n",
      "                  [--evaluation_strategy {EvaluationStrategy.NO,EvaluationStrategy.STEPS,EvaluationStrategy.EPOCH}]\n",
      "                  [--prediction_loss_only]\n",
      "                  [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                  [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                  [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                  [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                  [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                  [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
      "                  [--learning_rate LEARNING_RATE]\n",
      "                  [--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n",
      "                  [--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n",
      "                  [--max_grad_norm MAX_GRAD_NORM]\n",
      "                  [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                  [--max_steps MAX_STEPS]\n",
      "                  [--lr_scheduler_type {SchedulerType.LINEAR,SchedulerType.COSINE,SchedulerType.COSINE_WITH_RESTARTS,SchedulerType.POLYNOMIAL,SchedulerType.CONSTANT,SchedulerType.CONSTANT_WITH_WARMUP}]\n",
      "                  [--warmup_steps WARMUP_STEPS] [--logging_dir LOGGING_DIR]\n",
      "                  [--logging_first_step] [--logging_steps LOGGING_STEPS]\n",
      "                  [--save_steps SAVE_STEPS]\n",
      "                  [--save_total_limit SAVE_TOTAL_LIMIT] [--no_cuda]\n",
      "                  [--seed SEED] [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                  [--fp16_backend {auto,amp,apex}] [--local_rank LOCAL_RANK]\n",
      "                  [--tpu_num_cores TPU_NUM_CORES] [--tpu_metrics_debug]\n",
      "                  [--debug] [--dataloader_drop_last] [--eval_steps EVAL_STEPS]\n",
      "                  [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                  [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
      "                  [--disable_tqdm DISABLE_TQDM] [--no_remove_unused_columns]\n",
      "                  [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
      "                  [--load_best_model_at_end]\n",
      "                  [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
      "                  [--greater_is_better GREATER_IS_BETTER] [--ignore_data_skip]\n",
      "                  [--sharded_ddp] [--deepspeed DEEPSPEED]\n",
      "                  [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
      "                  [--adafactor]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model_name_or_path MODEL_NAME_OR_PATH\n",
      "                        The model checkpoint for weights initialization.Don't\n",
      "                        set if you want to train a model from scratch.\n",
      "  --model_type MODEL_TYPE\n",
      "                        If training from scratch, pass a model type from the\n",
      "                        list: layoutlm, distilbert, albert, bart, camembert,\n",
      "                        xlm-roberta, longformer, roberta, squeezebert, bert,\n",
      "                        mobilebert, flaubert, xlm, electra, reformer, funnel,\n",
      "                        mpnet, tapas\n",
      "  --config_name CONFIG_NAME\n",
      "                        Pretrained config name or path if not the same as\n",
      "                        model_name\n",
      "  --tokenizer_name TOKENIZER_NAME\n",
      "                        Pretrained tokenizer name or path if not the same as\n",
      "                        model_name\n",
      "  --cache_dir CACHE_DIR\n",
      "                        Where do you want to store the pretrained models\n",
      "                        downloaded from s3\n",
      "  --no_use_fast_tokenizer\n",
      "                        Whether to use one of the fast tokenizer (backed by\n",
      "                        the tokenizers library) or not.\n",
      "  --dataset_name DATASET_NAME\n",
      "                        The name of the dataset to use (via the datasets\n",
      "                        library).\n",
      "  --dataset_config_name DATASET_CONFIG_NAME\n",
      "                        The configuration name of the dataset to use (via the\n",
      "                        datasets library).\n",
      "  --train_file TRAIN_FILE\n",
      "                        The input training data file (a text file).\n",
      "  --validation_file VALIDATION_FILE\n",
      "                        An optional input evaluation data file to evaluate the\n",
      "                        perplexity on (a text file).\n",
      "  --overwrite_cache     Overwrite the cached training and evaluation sets\n",
      "  --max_seq_length MAX_SEQ_LENGTH\n",
      "                        The maximum total input sequence length after\n",
      "                        tokenization. Sequences longer than this will be\n",
      "                        truncated.\n",
      "  --preprocessing_num_workers PREPROCESSING_NUM_WORKERS\n",
      "                        The number of processes to use for the preprocessing.\n",
      "  --mlm_probability MLM_PROBABILITY\n",
      "                        Ratio of tokens to mask for masked language modeling\n",
      "                        loss\n",
      "  --line_by_line        Whether distinct lines of text in the dataset are to\n",
      "                        be handled as distinct sequences.\n",
      "  --pad_to_max_length   Whether to pad all samples to `max_seq_length`. If\n",
      "                        False, will pad the samples dynamically when batching\n",
      "                        to the maximum length in the batch.\n",
      "  --output_dir OUTPUT_DIR\n",
      "                        The output directory where the model predictions and\n",
      "                        checkpoints will be written.\n",
      "  --overwrite_output_dir\n",
      "                        Overwrite the content of the output directory.Use this\n",
      "                        to continue training if output_dir points to a\n",
      "                        checkpoint directory.\n",
      "  --do_train            Whether to run training.\n",
      "  --do_eval             Whether to run eval on the dev set.\n",
      "  --do_predict          Whether to run predictions on the test set.\n",
      "  --evaluation_strategy {EvaluationStrategy.NO,EvaluationStrategy.STEPS,EvaluationStrategy.EPOCH}\n",
      "                        The evaluation strategy to use.\n",
      "  --prediction_loss_only\n",
      "                        When performing evaluation and predictions, only\n",
      "                        returns the loss.\n",
      "  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n",
      "                        Batch size per GPU/TPU core/CPU for training.\n",
      "  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n",
      "                        Batch size per GPU/TPU core/CPU for evaluation.\n",
      "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
      "                        Deprecated, the use of `--per_device_train_batch_size`\n",
      "                        is preferred. Batch size per GPU/TPU core/CPU for\n",
      "                        training.\n",
      "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
      "                        Deprecated, the use of `--per_device_eval_batch_size`\n",
      "                        is preferred.Batch size per GPU/TPU core/CPU for\n",
      "                        evaluation.\n",
      "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
      "                        Number of updates steps to accumulate before\n",
      "                        performing a backward/update pass.\n",
      "  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\n",
      "                        Number of predictions steps to accumulate before\n",
      "                        moving the tensors to the CPU.\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        The initial learning rate for Adam.\n",
      "  --weight_decay WEIGHT_DECAY\n",
      "                        Weight decay if we apply some.\n",
      "  --adam_beta1 ADAM_BETA1\n",
      "                        Beta1 for Adam optimizer\n",
      "  --adam_beta2 ADAM_BETA2\n",
      "                        Beta2 for Adam optimizer\n",
      "  --adam_epsilon ADAM_EPSILON\n",
      "                        Epsilon for Adam optimizer.\n",
      "  --max_grad_norm MAX_GRAD_NORM\n",
      "                        Max gradient norm.\n",
      "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
      "                        Total number of training epochs to perform.\n",
      "  --max_steps MAX_STEPS\n",
      "                        If > 0: set total number of training steps to perform.\n",
      "                        Override num_train_epochs.\n",
      "  --lr_scheduler_type {SchedulerType.LINEAR,SchedulerType.COSINE,SchedulerType.COSINE_WITH_RESTARTS,SchedulerType.POLYNOMIAL,SchedulerType.CONSTANT,SchedulerType.CONSTANT_WITH_WARMUP}\n",
      "                        The scheduler type to use.\n",
      "  --warmup_steps WARMUP_STEPS\n",
      "                        Linear warmup over warmup_steps.\n",
      "  --logging_dir LOGGING_DIR\n",
      "                        Tensorboard log dir.\n",
      "  --logging_first_step  Log the first global_step\n",
      "  --logging_steps LOGGING_STEPS\n",
      "                        Log every X updates steps.\n",
      "  --save_steps SAVE_STEPS\n",
      "                        Save checkpoint every X updates steps.\n",
      "  --save_total_limit SAVE_TOTAL_LIMIT\n",
      "                        Limit the total amount of checkpoints.Deletes the\n",
      "                        older checkpoints in the output_dir. Default is\n",
      "                        unlimited checkpoints\n",
      "  --no_cuda             Do not use CUDA even when it is available\n",
      "  --seed SEED           random seed for initialization\n",
      "  --fp16                Whether to use 16-bit (mixed) precision (through\n",
      "                        NVIDIA Apex) instead of 32-bit\n",
      "  --fp16_opt_level FP16_OPT_LEVEL\n",
      "                        For fp16: Apex AMP optimization level selected in\n",
      "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
      "                        https://nvidia.github.io/apex/amp.html\n",
      "  --fp16_backend {auto,amp,apex}\n",
      "                        The backend to be used for mixed precision.\n",
      "  --local_rank LOCAL_RANK\n",
      "                        For distributed training: local_rank\n",
      "  --tpu_num_cores TPU_NUM_CORES\n",
      "                        TPU: Number of TPU cores (automatically passed by\n",
      "                        launcher script)\n",
      "  --tpu_metrics_debug   Deprecated, the use of `--debug` is preferred. TPU:\n",
      "                        Whether to print debug metrics\n",
      "  --debug               Whether to print debug metrics on TPU\n",
      "  --dataloader_drop_last\n",
      "                        Drop the last incomplete batch if it is not divisible\n",
      "                        by the batch size.\n",
      "  --eval_steps EVAL_STEPS\n",
      "                        Run an evaluation every X steps.\n",
      "  --dataloader_num_workers DATALOADER_NUM_WORKERS\n",
      "                        Number of subprocesses to use for data loading\n",
      "                        (PyTorch only). 0 means that the data will be loaded\n",
      "                        in the main process.\n",
      "  --past_index PAST_INDEX\n",
      "                        If >=0, uses the corresponding part of the output as\n",
      "                        the past state for next step.\n",
      "  --run_name RUN_NAME   An optional descriptor for the run. Notably used for\n",
      "                        wandb logging.\n",
      "  --disable_tqdm DISABLE_TQDM\n",
      "                        Whether or not to disable the tqdm progress bars.\n",
      "  --no_remove_unused_columns\n",
      "                        Remove columns not required by the model when using an\n",
      "                        nlp.Dataset.\n",
      "  --label_names LABEL_NAMES [LABEL_NAMES ...]\n",
      "                        The list of keys in your dictionary of inputs that\n",
      "                        correspond to the labels.\n",
      "  --load_best_model_at_end\n",
      "                        Whether or not to load the best model found during\n",
      "                        training at the end of training.\n",
      "  --metric_for_best_model METRIC_FOR_BEST_MODEL\n",
      "                        The metric to use to compare two different models.\n",
      "  --greater_is_better GREATER_IS_BETTER\n",
      "                        Whether the `metric_for_best_model` should be\n",
      "                        maximized or not.\n",
      "  --ignore_data_skip    When resuming training, whether or not to skip the\n",
      "                        first epochs and batches to get to the same training\n",
      "                        data.\n",
      "  --sharded_ddp         Whether or not to use sharded DDP training (in\n",
      "                        distributed training only).\n",
      "  --deepspeed DEEPSPEED\n",
      "                        Enable deepspeed and pass the path to deepspeed json\n",
      "                        config file (e.g. ds_config.json)\n",
      "  --label_smoothing_factor LABEL_SMOOTHING_FACTOR\n",
      "                        The label smoothing epsilon to apply (zero means no\n",
      "                        label smoothing).\n",
      "  --adafactor           Whether or not to replace Adam by Adafactor.\n"
     ]
    }
   ],
   "source": [
    "!python3 ../transformers/examples/language-modeling/run_mlm.py --help"
   ]
  },
  {
   "source": [
    "Let's download a French dataset and train it with cammem-base"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset allocine_dataset (/users/grad/frahimi/.cache/huggingface/datasets/allocine_dataset/allocine/1.0.0/bbee2ebb45a067891973b91ebdd40a93598d1e2dd5710b6714cdc2cd81d0ed65)\n",
      "Reusing dataset allocine_dataset (/users/grad/frahimi/.cache/huggingface/datasets/allocine_dataset/allocine/1.0.0/bbee2ebb45a067891973b91ebdd40a93598d1e2dd5710b6714cdc2cd81d0ed65)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"allocine\", split='train')\n",
    "\n",
    "allocine_txt = \" \" .join(dataset['review'])\n",
    "\n",
    "f = open(\"allocine.txt\", \"a\")\n",
    "f.write(allocine_txt)\n",
    "f.close()\n",
    "\n",
    "dataset_validation = load_dataset(\"allocine\", split='validation')\n",
    "\n",
    "validation_txt = \" \" .join(dataset_validation['review'])\n",
    "# len(validation_txt)\n",
    "\n",
    "ff = open(\"validation.txt\", \"a\")\n",
    "ff.write(validation_txt)\n",
    "ff.close()"
   ]
  },
  {
   "source": [
    "To run on your own training and validation files, use the following command:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../transformers/examples/language-modeling/run_mlm.py \\\n",
    "    --model_name_or_path camembert-base \\\n",
    "    --train_file allocine.txt \\\n",
    "    --validation_file validation.txt \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --output_dir /tmp/test-mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}