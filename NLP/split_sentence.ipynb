{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0b323bbe76accce5cb8b585882ab7f83a348dca44d37cb56a377991ed32b9cd38",
   "display_name": "Python 3.7.6  ('.env': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "b323bbe76accce5cb8b585882ab7f83a348dca44d37cb56a377991ed32b9cd38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "This notebooks tries to explore the answer to this [question](https://datascience.stackexchange.com/questions/93526/how-do-i-split-contents-in-a-text-that-would-include-two-or-more-different-theme) on Data Science stack exchange website. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Regex\n",
    "\n",
    "[Source](https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['This is an example sent.',\n",
       " 'The sentence splitter will split on sent markers.',\n",
       " 'Ohh really !',\n",
       " '!']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "split_into_sentences(example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "split_into_sentences(example)"
   ]
  },
  {
   "source": [
    "# NLTK"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The airlines have affected by Corona since march 2020 a crime has been detected in Noia village this morning']\n[' This is an example sent.', 'The sentence splitter will split on sent markers.', 'Ohh really !', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "example = \"The airlines have affected by Corona since march 2020 a crime has been detected in Noia village this morning\"\n",
    "print(sent_tokenize(example))\n",
    "\n",
    "example2 = ' This is an example sent. The sentence splitter will split on sent markers. Ohh really !!'\n",
    "print(sent_tokenize(example2))"
   ]
  },
  {
   "source": [
    "# Spacy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install spacy\n",
    "# ! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f950f727a90>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['This', 'tutorial', 'is', 'about', 'Natural', 'Language', 'Processing', 'in', 'Spacy', '.']\n"
     ]
    }
   ],
   "source": [
    "introduction_text = ('This tutorial is about Natural'\n",
    "    ' Language Processing in Spacy.')\n",
    "introduction_doc = nlp(introduction_text)\n",
    "# Extract tokens for the given doc\n",
    "print ([token.text for token in introduction_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gus Proto is a Python developer currently working for a London-based Fintech company.\nHe is interested in learning Natural Language Processing.\n"
     ]
    }
   ],
   "source": [
    "about_text = ('Gus Proto is a Python developer currently'\n",
    "              ' working for a London-based Fintech'\n",
    "              ' company. He is interested in learning'\n",
    "              ' Natural Language Processing.')\n",
    "about_doc = nlp(about_text)\n",
    "sentences = list(about_doc.sents)\n",
    "len(sentences)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print (sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " \nThis is an example sent.\nThe sentence splitter will split on sent markers.\nOhh really !!\n"
     ]
    }
   ],
   "source": [
    "about_doc = nlp(example2)\n",
    "sentences = list(about_doc.sents)\n",
    "len(sentences)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The airlines have affected by Corona since march 2020 a crime has been detected in Noia village this morning\n"
     ]
    }
   ],
   "source": [
    "about_doc = nlp(example)\n",
    "sentences = list(about_doc.sents)\n",
    "len(sentences)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}